{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "page_satus_conv1d_rnn_optuna",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sleepless-se/Colab/blob/master/page_satus_conv1d_rnn_optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SBfglvxvnvIc",
        "colab_type": "code",
        "outputId": "d7af7bf4-cf5a-4eab-cf8a-2c10f1d510bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4440
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get -q -y install swig \n",
        "!apt-get install mecab\n",
        "!apt-get install libmecab-dev\n",
        "!apt-get install mecab-ipadic-utf8\n",
        "!pip install mecab-python3\n",
        "!pip install optuna"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (855 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab2 mecab mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "0 upgraded, 5 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 16.5 MB of archives.\n",
            "After this operation, 219 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Fetched 16.5 MB in 2s (8,897 kB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 132114 files and directories currently installed.)\n",
            "Preparing to unpack .../libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/juman-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 308 kB of archives.\n",
            "After this operation, 3,132 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Fetched 308 kB in 1s (249 kB/s)\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "(Reading database ... 132239 files and directories currently installed.)\n",
            "Preparing to unpack .../libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  mecab-ipadic\n",
            "The following NEW packages will be installed:\n",
            "  mecab-ipadic mecab-ipadic-utf8\n",
            "0 upgraded, 2 newly installed, 0 to remove and 10 not upgraded.\n",
            "Need to get 12.1 MB of archives.\n",
            "After this operation, 54.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 12.1 MB in 2s (4,940 kB/s)\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "(Reading database ... 132247 files and directories currently installed.)\n",
            "Preparing to unpack .../mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Collecting mecab-python3\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/48/295efe525df40cbc2173748eb869290e81a57e835bc41f6d3834fc5dad5f/mecab-python3-0.996.1.tar.gz\n",
            "Building wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/73/71/4f/63a79925b5e9bb38932043917cc60140beb8022ac14a952b1e\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.1\n",
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/8a/3d86da5e3c03aafc14e186d66998aa9fbc97d45a578045482888455eecd1/optuna-0.9.0.tar.gz (64kB)\n",
            "\u001b[K    100% |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from optuna) (1.11.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from optuna) (3.6.6)\n",
            "Collecting cliff (from optuna)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/d5/e811665fde537964dd355c6ce38bd1308ef0ebe966fb81b3f2c6810c3845/cliff-2.14.1-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 7.0MB/s \n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading https://files.pythonhosted.org/packages/68/4d/892728b0c14547224f0ac40884e722a3d00cb54e7a146aea0b3186806c9e/colorlog-4.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from optuna) (0.22.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.3.1)\n",
            "Collecting cmd2!=0.8.3; python_version >= \"3.0\" (from cliff->optuna)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/ca/35731e97fb6615e0e448482a4020e9cd8e38f22f1eb66d9209f8055d31eb/cmd2-0.9.11-py3-none-any.whl (83kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.1.3)\n",
            "Collecting stevedore>=1.20.0 (from cliff->optuna)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/dc/6ee92bccfe3c0448786b30b693e6060d62ec8c4a3ec9a287bac1c1a8d8c9/stevedore-1.30.1-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 17.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->optuna) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->optuna) (2.5.3)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3; python_version >= \"3.0\"->cliff->optuna) (0.1.7)\n",
            "Collecting pyperclip>=1.5.27 (from cmd2!=0.8.3; python_version >= \"3.0\"->cliff->optuna)\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/0f/4eda562dffd085945d57c2d9a5da745cfb5228c02bc90f2c74bbac746243/pyperclip-1.7.0.tar.gz\n",
            "Collecting colorama (from cmd2!=0.8.3; python_version >= \"3.0\"->cliff->optuna)\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3; python_version >= \"3.0\"->cliff->optuna) (19.1.0)\n",
            "Building wheels for collected packages: optuna, pyperclip\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7d/52/a4/fe869e238d0d6b290dcb8ec7415f69ae17ddb38c1bd080a220\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/92/f0/ac/2ba2972034e98971c3654ece337ac61e546bdeb34ca960dc8c\n",
            "Successfully built optuna pyperclip\n",
            "Installing collected packages: pyperclip, colorama, cmd2, stevedore, cliff, colorlog, optuna\n",
            "Successfully installed cliff-2.14.1 cmd2-0.9.11 colorama-0.4.1 colorlog-4.0.2 optuna-0.9.0 pyperclip-1.7.0 stevedore-1.30.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "utt5O8Q-akpv",
        "colab_type": "code",
        "outputId": "b85740a0-d8f1-4672-c9ef-e73a578b8a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import sys, pickle, os, json, keras, time, re\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.datasets import boston_housing\n",
        "from keras.models import Model,model_from_json\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.metrics import categorical_accuracy\n",
        "from scipy.stats import zscore\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import MeCab\n",
        "import optuna\n",
        "mecab = MeCab.Tagger(\"-Owakati\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "d2OekP5Mavkp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_text_tokenizer(tokenizer,file_name):\n",
        "  # saving\n",
        "  with open(file_name+\".pickle\", 'wb') as handle:\n",
        "      pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_text_tokenizer(file_name):\n",
        "  # loading\n",
        "  with open(file_name+\".pickle\", 'rb') as handle:\n",
        "      return pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9yfdxFnPazub",
        "colab_type": "code",
        "outputId": "4da754d0-b6f0-45b8-d7c8-8bb60f3c2894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "base_dir = \"/content/gdrive/My Drive/data/status_filter/\"\n",
        "!ls \"/content/gdrive/My Drive/data/status_filter/\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "data\t\t  keras_model.hdf5  page_st.csv\n",
            "error.csv\t  keras_model.json  text_token.pickle\n",
            "error_domain.csv  logs\t\t    weights.best.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IYX9sftFbANX",
        "colab_type": "code",
        "outputId": "d22235dd-e070-4b4a-8f36-d12174f55074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "page_file_name = 'page_st.csv'\n",
        "target_columns = ['label','text']\n",
        "dtype ={'label':str,'text':str} \n",
        "raw_df = pd.read_csv(base_dir + page_file_name,sep=',',error_bad_lines=False, dtype=dtype,engine='python')\n",
        "raw_df = raw_df[~raw_df.duplicated()]\n",
        "print('raw_df',raw_df.shape)\n",
        "print(raw_df.columns)\n",
        "print(raw_df['label'].value_counts())\n",
        "raw_df.to_csv(base_dir+'page_st.csv',index=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "raw_df (2452, 3)\n",
            "Index(['label', 'url', 'text'], dtype='object')\n",
            "no        1463\n",
            "stock      692\n",
            "list       139\n",
            "top         96\n",
            "block       25\n",
            "login       25\n",
            "except      12\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9jmnQy0rY4Dt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def change_label(label):\n",
        "  if label != 'stock' :label = 'no'\n",
        "  return label\n",
        "\n",
        "raw_df['label'] = raw_df['label'].apply(lambda x : change_label(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Hk0LB_AcriH",
        "colab_type": "code",
        "outputId": "c0f2a399-c823-4e6e-ec70-1ad26d099db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "raw_df = raw_df.dropna()\n",
        "raw_df = raw_df.sample(frac=1)\n",
        "print(raw_df[\"label\"].value_counts())\n",
        "print('raw_df',raw_df.shape)\n",
        "print(raw_df.columns)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no       1760\n",
            "stock     692\n",
            "Name: label, dtype: int64\n",
            "raw_df (2452, 3)\n",
            "Index(['label', 'url', 'text'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4qCpuLi3E1nx",
        "colab_type": "code",
        "outputId": "ea58cdba-5f4f-4691-f9f1-991e3b41f188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "text = 'sorry we can t this product page 　  　re sold out 404 whoops our bad it gone out of stock 切れ 在庫 なし 完売 ました 売り 切れ ページ 見つかり ません この 製品 もう 入手 できません'\n",
        "def prepro(text:str)->list:\n",
        "  line = \"\"\n",
        "  text = mecab.parse(str(text)).replace(\"↵\",\" \")\n",
        "  text = re.sub('{.*}','',text)\n",
        "  words = text.lower()\n",
        "  words = re.sub('\\s{2,}',\" \",words)\n",
        "  return words\n",
        "prepro(text)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sorry we can t this product page re sold out 404 whoops our bad it gone out of stock 切れ 在庫 なし 完売 まし た 売り 切れ ページ 見つかり ませ ん この 製品 もう 入手 でき ませ ん '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "morxFLPSjdfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_df['edit_text'] = raw_df['text'].apply(lambda x : prepro(x))\n",
        "token = keras.preprocessing.text.Tokenizer(num_words=250)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6FnQdLw6nDaN",
        "colab_type": "code",
        "outputId": "77c45a4f-a41c-44b0-b66c-13e66db417f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "raw_df['edit_text'].values[0:1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['access denied you don \\' t have permission to access \" http :// www 1 . bloomingdales . com / shop / product / marc - by - marc - jacobs - domo - arigato - mini - packrat - color - block - backpack ?\" on this server . reference # 18 . b 7 c 733 b 8 . 1551963353 . e 1 d 2 d 8 d '],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "DS8FDbMbhyal",
        "colab_type": "code",
        "outputId": "556b4ac5-2003-4ce1-f550-8252df36967a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "raw_df['edit_text'] = raw_df['text'].apply(lambda x : prepro(x))\n",
        "regex = \"^(sorry.? (we can.t|this (product|page)))|^(sorry we.re sold out)|^(404|whoops, our bad...|it.s gone|out of stock|sold out|在庫切れ|在庫なし|完売しました|売り切れ|ページが見つかりません.*|この製品はもう入手できない.|ci scusiamo per l'inconveniente.|sorry!|back to our favs)$|(((product|page|item|requested) (is |was )?)(not found|no longer|sold out|out of stock|not available))|(re (sorry|sold out)|(ご購入いただけません)|(404 error))|^(sold out\\\\s*){3,}|ARTICLE.*EN LIGNE|^so sorry.*|404 page not|Don.t Cry|.*is out of stock.|articolo non disponibile\";\n",
        "p = re.compile(regex)\n",
        "raw_df['flag'] = raw_df['text'].apply(lambda x : 0 if p.match(x) == None else 1)\n",
        "raw_df['edit_text'] = raw_df['edit_text'].fillna(\"\")\n",
        "\n",
        "token = keras.preprocessing.text.Tokenizer(num_words=250)\n",
        "token.fit_on_texts(raw_df['edit_text'].values)\n",
        "save_text_tokenizer(token,base_dir + 'text_token')\n",
        "X = token.texts_to_matrix(raw_df['edit_text'].values)\n",
        "flag = np.array(raw_df['flag'])\n",
        "flag = flag.reshape(len(flag),1)\n",
        "print('flag',flag.shape,'X',X.shape)\n",
        "# X = np.concatenate((flag,X),axis=1)\n",
        "\n",
        "# X = flag\n",
        "# text_len = len(X[0])\n",
        "# print(f'text_len:{text_len} samples:{len(X)}')\n",
        "print('word_count',len(token.word_counts))\n",
        "# print(token.word_counts)\n",
        "# print(token.word_index)\n",
        "X = np.array(X)\n",
        "print('X',X.shape)\n",
        "\n",
        "max_len = 1000\n",
        "X_seq = token.texts_to_sequences(raw_df['text'].values)\n",
        "X_seq = sequence.pad_sequences(X_seq,maxlen=max_len)\n",
        "max_len = int(X_seq.shape[1])\n",
        "print('max_len',max_len)\n",
        "# X_seq = X_seq.reshape(X_seq.shape[0],1,X_seq.shape[1])\n",
        "print('X_seq',X_seq.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flag (2452, 1) X (2452, 250)\n",
            "word_count 27729\n",
            "X (2452, 250)\n",
            "max_len 1000\n",
            "X_seq (2452, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "clu2Wzuiz4-n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kg-JQooXPZlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index2word =  {i+1:key for i, key in enumerate(token.word_index)}\n",
        "# index2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-TS15_A8jZSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# label_token = keras.preprocessing.text.Tokenizer()\n",
        "# label_token.fit_on_texts(raw_df['label'].values)\n",
        "# y = label_token.texts_to_matrix(raw_df['label'].values)\n",
        "# label_len = len(label_token.word_counts) + 1\n",
        "raw_df['true'] = raw_df['label'].apply(lambda x : 1 if x == 'stock' else 0)\n",
        "y = to_categorical(raw_df['true'].values)\n",
        "# print('word_index',label_token.word_index)\n",
        "# print('label_len',label_len)\n",
        "# print('y shape',y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tRiqyNRmHk5F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XKYnceKz_emV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def objective(trial):\n",
        "  \n",
        "#     k_layer = trial.suggest_int('k_layer', 0, 4)\n",
        "#     l_layer = trial.suggest_int('l_layer', 0, 4)\n",
        "#     n_layer = trial.suggest_int('n_layer', 0, 4)\n",
        "#     m_layer = trial.suggest_int('m_layer', 0, 4)\n",
        "    \n",
        "    \n",
        "#     embed_size = int(trial.suggest_discrete_uniform('embed_size', 800, 2000, 100))\n",
        "#     embed_dim = int(trial.suggest_discrete_uniform('embed_dim', 32, 256, 1))\n",
        "    \n",
        "    conv1_units = int(trial.suggest_discrete_uniform('conv1_units', 12, 256, 1))\n",
        "    conv2_units = int(trial.suggest_discrete_uniform('conv2_units', 12, 256, 1))\n",
        "    conv1_pool = int(trial.suggest_discrete_uniform('conv1_pool', 2, 12, 1)) \n",
        "    conv2_pool = int(trial.suggest_discrete_uniform('conv2_pool', 2, 12, 1)) \n",
        "#     max_pool1_size = int(trial.suggest_discrete_uniform('max_pool1_size', 2, 256, 1)) \n",
        "#     max_pool2_size = int(trial.suggest_discrete_uniform('max_pool2_size', 2, 256, 1)) \n",
        "\n",
        "    lstm_units = int(trial.suggest_discrete_uniform('lstm_units', 2, 256, 1)) \n",
        "#     lstm2_ini = trial.suggest_categorical('lstm2_ini', ['zeros', 'ones', 'orthogonal']) \n",
        "    dropout_rate1 = trial.suggest_uniform('dropout_rate', 0, 1)\n",
        "    dropout_rate2 = trial.suggest_uniform('dropout_rate', 0, 1)\n",
        "    lstm_dropout = trial.suggest_uniform('dropout_rate', 0, 1)\n",
        "    \n",
        "    dense1_units = int(trial.suggest_discrete_uniform('dense1_units', 4, 256, 1))\n",
        "    conv1_active = trial.suggest_categorical('conv1_active', ['relu', 'sigmoid', 'softmax']) \n",
        "    conv2_active = trial.suggest_categorical('conv2_active', ['relu', 'sigmoid', 'softmax']) \n",
        "#     last_dens_active = trial.suggest_categorical('last_dens_active', ['sigmoid', 'softmax']) \n",
        "#     dropout_rate2 = trial.suggest_uniform('dropout_rate', 0, 1)\n",
        "\n",
        "    \n",
        "    # create model\n",
        "  \n",
        "    embedding_layer = layers.Embedding(1500,output_dim=64)\n",
        "    input_seq = layers.Input(shape=(max_len,))\n",
        "    x = embedding_layer(input_seq)\n",
        "    x = layers.Conv1D(conv1_units,9)(x)\n",
        "    x = layers.Activation(conv1_active)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate1)(x)\n",
        "    x = layers.MaxPooling1D(9)(x)\n",
        "    x = layers.Conv1D(conv1_units,9)(x)\n",
        "    x = layers.Activation(conv2_active)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate2)(x)\n",
        "    x = layers.MaxPooling1D(9)(x)\n",
        "    \n",
        "    x = layers.Bidirectional(layers.CuDNNLSTM(lstm_units, return_sequences=True, kernel_initializer='Orthogonal'))(x) \n",
        "    x = layers.Dropout(lstm_dropout)(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(2,activation='softmax')(x)\n",
        "  \n",
        "\n",
        "#     model = get_model(embed_size,embed_dim,l_layer,n_layer,m_layer, dens_active, last_dens_active,last_dens_units,conv_units,conv_pool,max_pool_size,lstm1_units,lstm2_units, dropout_rate1, dropout_rate2,lstm1_ini,letm2_ini)\n",
        "    model = Model(input_seq,x)\n",
        "    model.compile(loss='categorical_crossentropy',optimizer ='adam',metrics=['categorical_accuracy'])\n",
        "    \n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, \n",
        "                        verbose=0,\n",
        "                        epochs=10,\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        batch_size=128)\n",
        "\n",
        "    # 学習モデルの保存\n",
        "    model_json = model.to_json()\n",
        "    with open(base_dir+'keras_model.json', 'w') as f_model:\n",
        "        f_model.write(model_json)\n",
        "    model.save_weights(base_dir+'keras_model.hdf5')\n",
        "\n",
        "    # 最小値探索なので\n",
        "    return -np.amax(history.history['val_categorical_accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SpteN90EG29f",
        "colab_type": "code",
        "outputId": "e053fe3f-45dd-454f-ddb0-b3c32f1bb700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=500)\n",
        "print('best_params')\n",
        "print(study.best_params)\n",
        "print('-1 x best_value')\n",
        "print(-study.best_value)\n",
        "\n",
        "print('\\n --- sorted --- \\n')\n",
        "sorted_best_params = sorted(study.best_params.items(), key=lambda x : x[0])\n",
        "for i, k in sorted_best_params:\n",
        "    print(i + ' = ' + str(k))\n",
        "# history\n",
        "hist_df = study.trials_dataframe()\n",
        "hist_df.to_csv(base_dir+\"boston_svr.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[I 2019-03-21 02:26:38,404] Finished trial#0 resulted in value: -0.837067208076446. Current best value is -0.837067208076446 with parameters: {'conv1_units': 253.0, 'conv2_units': 211.0, 'conv1_pool': 5.0, 'conv2_pool': 8.0, 'lstm_units': 75.0, 'dropout_rate': 0.744713382088526, 'dense1_units': 248.0, 'conv1_active': 'sigmoid', 'conv2_active': 'softmax'}.\n",
            "[I 2019-03-21 02:26:55,035] Finished trial#1 resulted in value: -0.7800407314980345. Current best value is -0.837067208076446 with parameters: {'conv1_units': 253.0, 'conv2_units': 211.0, 'conv1_pool': 5.0, 'conv2_pool': 8.0, 'lstm_units': 75.0, 'dropout_rate': 0.744713382088526, 'dense1_units': 248.0, 'conv1_active': 'sigmoid', 'conv2_active': 'softmax'}.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "F8EI6x8WyS5_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I-uTbtZYDZ2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "0.8778004041757215\n",
        "\n",
        " --- sorted --- \n",
        "\n",
        "conv1_active = relu\n",
        "conv1_pool = 12.0\n",
        "conv1_units = 69.0\n",
        "conv2_active = sigmoid\n",
        "conv2_pool = 6.0\n",
        "conv2_units = 113.0\n",
        "dense1_units = 42.0\n",
        "dropout_rate = 0.015408279269197522\n",
        "embed_dim = 173.0\n",
        "embed_size = 1300.0\n",
        "lstm_units = 92.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yG1hayURCuVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "load_model = keras.models.load_model(base_dir +'keras_model.hdf5')\n",
        "score = load_model.evaluate(X_test,y_test)\n",
        "print('Test best loss:', score[0])\n",
        "print('Test best accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IXbTQt0jMEcO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  activation = 'sigmoid'\n",
        "  dropout_rate = 0.044713558274572573\n",
        "  lstm_units = 178.0\n",
        "  m_layer = 1\n",
        "  mid_units = 96.0\n",
        "  n_layer = 3\n",
        "  optimizer = 'adam'\n",
        "  embedding_size = 1500\n",
        "  \n",
        "  input_seq = layers.Input(shape = (784,))\n",
        "  embedding_layer = layers.Embedding(embedding_size,output_dim=64)\n",
        "  second_input_encoded = embedding_layer(input_seq) \n",
        "  for i in range(3):\n",
        "    x = layers.Conv1D(mid_units, 9, activation=activation)(second_input_encoded)\n",
        "    x = layers.MaxPooling1D(9)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "  x = layers.Bidirectional(layers.CuDNNLSTM(8, return_sequences=True, kernel_initializer='Orthogonal'))(x) \n",
        "  x = layers.Dropout(dropout_rate)(x)\n",
        "  rnn_layer = layers.Bidirectional(layers.CuDNNLSTM(4, return_sequences=True, kernel_initializer='Orthogonal'))(x) \n",
        "  rnn_layer = layers.Flatten()(rnn_layer)\n",
        "  output = layers.Dense(2,activation='softmax')(rnn_layer)\n",
        "  model = Model(inputs=input_seq, outputs=output)\n",
        "  model.compile(loss='categorical_crossentropy',optimizer ='adam',metrics=['categorical_accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eoi59cXgyouE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 150\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# stop when score is not improve\n",
        "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=25)\n",
        "\n",
        "# save best score\n",
        "filepath = base_dir+\"weights.best.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "# TensorBoard\n",
        "# tsb = TensorBoard(log_dir=base_dir+'logs')\n",
        "\n",
        "# reduce \n",
        "reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.01)\n",
        "callbacks_list = [checkpoint,early_stopping,reduce]\n",
        "\n",
        "# model = get_model(0.4)\n",
        "# history = model.fit(X_train, y_train, validation_split=0.2,verbose=1,epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=callbacks_list)\n",
        "# history_df = pd.DataFrame(history.history)\n",
        "# history_df.plot()\n",
        "# scores = []\n",
        "# historys = []\n",
        "# kf = KFold(n_splits=4, random_state=None, shuffle=False)\n",
        "# for train_index, test_index in kf.split(X):\n",
        "#   model = get_model()\n",
        "#   train_data = [X[train_index], X_seq[train_index]]\n",
        "#   test_data = [X[test_index], X_seq[test_index]]\n",
        "#   history = model.fit(train_data, y[train_index], validation_split=0.2,verbose=0,epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=callbacks_list)\n",
        "#   historys.append(history)\n",
        "#   score = model.evaluate(test_data,y[test_index])\n",
        "#   print(score)\n",
        "#   scores.append(score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "04i2OtQOMivV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "history = model.fit(X_train, y_train,validation_split=0.2,verbose=0,epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=callbacks_list)\n",
        "load_model = keras.models.load_model(filepath)\n",
        "score = load_model.evaluate(X_test, y_test)\n",
        "print('score',score[1]*100,\"%\")\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[['val_categorical_accuracy','categorical_accuracy']].plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZBy8X3r-eAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = [512,1024,1500,2048]\n",
        "for param in params:\n",
        "  model = get_model(param)\n",
        "  history = model.fit(X_train, y_train,validation_split=0.2,verbose=0,epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=callbacks_list)\n",
        "  load_model = keras.models.load_model(filepath)\n",
        "  score = load_model.evaluate(X_test, y_test)\n",
        "  print('param',param)\n",
        "  print('score',score[1]*100,\"%\")\n",
        "  history_df = pd.DataFrame(history.history)\n",
        "  history_df[['val_categorical_accuracy','categorical_accuracy']][35:].plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6dPUTU-1AbAx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "load_model = keras.models.load_model(filepath)\n",
        "score = load_model.evaluate(X_test,y_test)\n",
        "print('Test best loss:', score[0])\n",
        "print('Test best accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5vQ4yDCxLAnf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-okXHMaAxjbM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"KFlod\")\n",
        "print(\"Sample\",len(X))\n",
        "print(\"Score\",pd.DataFrame(scores).mean()[1]*100)\n",
        "print(\"Std\",pd.DataFrame(scores).std()[1]*100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6CjnyJDeJ_h5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "SNSZcbWuvHj8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(historys)):\n",
        "  df = pd.DataFrame(historys[i].history)\n",
        "#   df.filter(regex=('loss*')).plot()\n",
        "  df.filter(regex=('accuracy$')).plot()\n",
        "  \n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mK7uMG1B3Lno",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history.keys())\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['val_loss', 'loss'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['val_acc', 'acc'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T6TY-R8VGeJL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(test_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZSu4n9-iGpwA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_df['pred'] = load_model.predict(X).argmax(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FNg-NDVtG7CW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "error_df = raw_df.query(\"true != pred\")\n",
        "print(len(error_df))\n",
        "error_df[['label','true','pred','url','text']].to_csv(base_dir+'error.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jfq72bVOr1_K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4uCPsUliMR-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def df_to_X(raw_df:pd.DataFrame):\n",
        "  raw_df['edit_text'] = raw_df['text'].apply(lambda x : prepro(x))\n",
        "  regex = \"^(sorry.? (we can.t|this (product|page)))|^(sorry we.re sold out)|^(404|whoops, our bad...|it.s gone|out of stock|sold out|在庫切れ|在庫なし|完売しました|売り切れ|ページが見つかりません.*|この製品はもう入手できない.|ci scusiamo per l'inconveniente.|sorry!|back to our favs)$|(((product|page|item|requested) (is |was )?)(not found|no longer|sold out|out of stock|not available))|(re (sorry|sold out)|(ご購入いただけません)|(404 error))|^(sold out\\\\s*){3,}|ARTICLE.*EN LIGNE|^so sorry.*|404 page not|Don.t Cry|.*is out of stock.|articolo non disponibile\";\n",
        "  p = re.compile(regex)\n",
        "  raw_df['flag'] = raw_df['text'].apply(lambda x : 0 if p.match(x) == None else 1)\n",
        "  raw_df['edit_text'] = raw_df['edit_text'].fillna(\"\")\n",
        "\n",
        "  save_text_tokenizer(token,base_dir + 'text_token')\n",
        "  X = token.texts_to_matrix(raw_df['edit_text'].values)\n",
        "  flag = np.array(raw_df['flag'])\n",
        "  flag = flag.reshape(len(flag),1)\n",
        "#   print('flag',flag.shape,'X',X.shape)\n",
        "  # X = np.concatenate((flag,X),axis=1)\n",
        "\n",
        "  # X = flag\n",
        "  # text_len = len(X[0])\n",
        "  # print(f'text_len:{text_len} samples:{len(X)}')\n",
        "#   print('word_count',len(token.word_counts))\n",
        "#   print(token.word_counts)\n",
        "#   print(token.word_index)\n",
        "  X = np.array(X)\n",
        "  return X\n",
        "\n",
        "def get_split_num(text:str,num:int)->int:\n",
        "    max_len = int(len(text))\n",
        "    return int(max_len / num)\n",
        "  \n",
        "split = 10\n",
        "df = pd.DataFrame()\n",
        "pred_df = pd.DataFrame()\n",
        "result = []\n",
        "for i in range(split):\n",
        "  df['text'] = raw_df['text'].apply(lambda x : x[get_split_num(x,split)*i:get_split_num(x,split)*(i+1)])\n",
        "  X = df_to_X(df)\n",
        "  first = load_model.predict(X).argmax(axis=1)\n",
        "  result.append(first)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VV8U5fZjpFm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.array(result)\n",
        "X = X.T\n",
        "print(X.shape)\n",
        "X = X.reshape(len(X),split,1)\n",
        "print(X.shape)\n",
        "print(X[0:10:],y[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wEdzO6yDld95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm_model = models.Sequential()\n",
        "lstm_model.add(layers.LSTM(1,input_shape=(split,1)))\n",
        "# lstm_model.add(layers.Flatten())\n",
        "lstm_model.add(layers.Dense(2,activation='relu'))\n",
        "lstm_model.compile(loss='categorical_crossentropy',optimizer ='adam',metrics=['categorical_accuracy'])\n",
        "history = lstm_model.fit(X,y,epochs=5,validation_split=0.2,batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HMvIAmjErUsb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = lstm_model.predict(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWaCV5Bwv6yo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred.argmax(axis=1)[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXZA9fKlwElQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.array(y).argmax(axis=1)[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KPRuFDZzwvpt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}