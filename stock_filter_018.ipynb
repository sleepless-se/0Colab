{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stock_filter_018",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sleepless-se/Abstract/blob/master/stock_filter_018.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "-7qTwpLv8PlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##バリエーションだけを抽出する\n"
      ]
    },
    {
      "metadata": {
        "id": "1XOCX2tjDxSU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3114
        },
        "outputId": "18acaef2-4d51-43da-ed19-d911a7592863"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!apt-get install mecab\n",
        "!apt-get install libmecab-dev\n",
        "!apt-get install mecab-ipadic-utf8\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab2 mecab mecab-jumandic mecab-jumandic-utf8 mecab-utils\n",
            "0 upgraded, 5 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 16.5 MB of archives.\n",
            "After this operation, 219 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Fetched 16.5 MB in 0s (47.5 MB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 21063 files and directories currently installed.)\n",
            "Preparing to unpack .../libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/juman-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 308 kB of archives.\n",
            "After this operation, 3,132 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Fetched 308 kB in 0s (3,753 kB/s)\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "(Reading database ... 21188 files and directories currently installed.)\n",
            "Preparing to unpack .../libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  mecab-ipadic\n",
            "The following NEW packages will be installed:\n",
            "  mecab-ipadic mecab-ipadic-utf8\n",
            "0 upgraded, 2 newly installed, 0 to remove and 4 not upgraded.\n",
            "Need to get 12.1 MB of archives.\n",
            "After this operation, 54.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 12.1 MB in 0s (42.5 MB/s)\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "(Reading database ... 21196 files and directories currently installed.)\n",
            "Preparing to unpack .../mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/e9/bbf5fc790a2bedd96fbaf47a84afa060bfb0b3e0217e5f64b32bd4bbad69/mecab-python3-0.7.tar.gz (41kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Running setup.py bdist_wheel for mecab-python3 ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/4c/07/3a/5f22ccc9f381f3bc01fa023202061cd1e0e9af855292f005dd\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aIS_hwjgEHJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8569df5-1066-49df-f001-0b4940b1d222"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import MeCab\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "import MeCab\n",
        "import pickle\n",
        "from keras.datasets import boston_housing\n",
        "from keras.models import Model,model_from_json\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import time\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.metrics import categorical_accuracy\n",
        "import pickle\n",
        "\n",
        "    \n",
        "mecab = MeCab.Tagger (\"-Owakati\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2SR7Y4aK_fCt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Tokenizerの読み書き用関数"
      ]
    },
    {
      "metadata": {
        "id": "lJADBVEs_f2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def save_text_tokenizer(tokenizer,file_name):\n",
        "  # saving\n",
        "  with open(file_name+\".pickle\", 'wb') as handle:\n",
        "      pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_text_tokenizer(file_name):\n",
        "  # loading\n",
        "  with open(file_name+\".pickle\", 'rb') as handle:\n",
        "      return pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cOpL_NPO_YbM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ドライブをマウント"
      ]
    },
    {
      "metadata": {
        "id": "31y-m09d9h9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "f467df52-eb76-4847-e218-0ab8008bcf1d"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "base_dir = \"/content/gdrive/My Drive/data/stock_filter/018/\"\n",
        "!ls \"/content/gdrive/My Drive/data/stock_filter/018/\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "alt_token.pickle    model.json\t     none_label.csv\t vari_label.csv\n",
            "href_token.pickle   none_label0.csv  src_token.pickle\t weights.best.hdf5\n",
            "iid_token.pickle    none_label2.csv  style_token.pickle\n",
            "label_token.pickle  none_label3.csv  tag_token.pickle\n",
            "model.h5\t    none_label4.csv  text_token.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9tvSFM8TsdxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##学習データの読み込み\n"
      ]
    },
    {
      "metadata": {
        "id": "Xc_T19h7omO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7f667498-1d51-4c4c-8275-a3ec63b77105"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# dtype ={\"x\":int,\"p_x\":int,\"pp_x\":int,\"y\":int,\"p_y\":int,\"pp_y\":int,\"width\":int,\"p_width\":int,\"pp_width\":int,\"height\":int,\"p_height\":int,\"pp_height\":int,\"iid\": str,\"style\": str, \"text\": str, \"p_text\": str} \n",
        "dtype ={\"style\": str, \"text\": str, \"p_text\": str} \n",
        "file_name = 'vari_label.csv'\n",
        "stock_df = pd.read_csv(base_dir+file_name,sep=',',error_bad_lines=False, dtype=dtype,engine='python')\n",
        "# print(stock_df.head(10))\n",
        "print(\"vari shape\",stock_df.shape)\n",
        "\n",
        "file_name = 'none_label5.csv'\n",
        "null_df = pd.read_csv(base_dir+file_name,sep=',',error_bad_lines=False, dtype=dtype,engine='python')\n",
        "null_df[\"label\"] = \"skip\"\n",
        "print(\"null shape\",null_df.shape)\n",
        "\n",
        "df = stock_df.append(null_df, ignore_index=True)\n",
        "df.columns\n",
        "df.shape\n",
        "# print(df.head(10))\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vari shape (1489, 31)\n",
            "null shape (3000, 31)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4489, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "dHGqZOooqWTj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 分かち書きをしたりするテキストの前処理"
      ]
    },
    {
      "metadata": {
        "id": "Uyt9XQtsSndr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def to_df(token,se):\n",
        "  matrix = token.texts_to_matrix(se,mode='count')\n",
        "  temp_df = pd.DataFrame(matrix)\n",
        "  return temp_df\n",
        "\n",
        "def add_space_front_uppercase(text:str)->str:\n",
        "  return ''.join(' ' + char if char.isupper() else char.strip() for char in text).strip()\n",
        "\n",
        "def add_space_color_size(text:str)->str:\n",
        "  return text.replace(\"colour\",\"color\").replace(\"color\",\" color \").replace(\"size\",\" size \")\n",
        "\n",
        "def wakati(text)->str:\n",
        "  text = str(text)\n",
        "  if len(text)>3:\n",
        "    text = mecab.parse(text)\n",
        "  return replace_numbers_to_0(text)\n",
        "\n",
        "def replace_numbers_to_0(text:str)->str:\n",
        "  return re.sub(r'[0-9]+', \"0\",str(text))\n",
        "\n",
        "def pre_text(df):\n",
        "  wakachi_fileds = ['alt', 'href', 'iid','src', 'style', 'tag', 'text',\n",
        "                    'p_iid', 'p_style', 'p_tag','p_text', 'pp_iid', 'pp_style', 'pp_tag']\n",
        "  df[wakachi_fileds] = df[wakachi_fileds].applymap(lambda x : wakati(x)).fillna(\"\")\n",
        "\n",
        "  add_space_fileds = [\"iid\",\"style\",\"p_iid\",\"p_style\",\"pp_iid\",\"pp_style\"]\n",
        "  df[add_space_fileds] = df[add_space_fileds].applymap(lambda x : add_space_front_uppercase(x))\n",
        "  df[add_space_fileds] = df[add_space_fileds].applymap(lambda x : add_space_color_size(x))\n",
        "  return df                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-GAHD2g45ge9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "b90cc94c-0841-424f-899b-cc2952664466"
      },
      "cell_type": "code",
      "source": [
        "df = pre_text(df)\n",
        "df.dtypes"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id             int64\n",
              "label         object\n",
              "tag           object\n",
              "iid           object\n",
              "style         object\n",
              "x              int64\n",
              "y              int64\n",
              "height         int64\n",
              "width          int64\n",
              "page_id        int64\n",
              "text          object\n",
              "p_text        object\n",
              "p_height       int64\n",
              "p_iid         object\n",
              "p_style       object\n",
              "p_tag         object\n",
              "p_width        int64\n",
              "p_x            int64\n",
              "p_y            int64\n",
              "href          object\n",
              "disable      float64\n",
              "pp_tag        object\n",
              "pp_style      object\n",
              "src           object\n",
              "pp_height      int64\n",
              "pp_iid        object\n",
              "pp_width       int64\n",
              "pp_x           int64\n",
              "pp_y           int64\n",
              "alt           object\n",
              "pp_text       object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "S3c8ffaYr7ub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 学習データ全体の前処理"
      ]
    },
    {
      "metadata": {
        "id": "eHXfd2SgDIJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_data(data):\n",
        "  df = pd.DataFrame(data)\n",
        "  df = pre_text(df)\n",
        "\n",
        "  fields = ['height', 'width', 'x', 'y',  'p_x', 'p_y', 'p_width','p_height','pp_height', 'pp_width', 'pp_x', 'pp_y']\n",
        "  X = df[fields]\n",
        "\n",
        "  X = pd.concat([X, to_df(tag_token,df[\"tag\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(tag_token,df[\"p_tag\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(tag_token,df[\"pp_tag\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(style_token,df[\"style\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(style_token,df[\"p_style\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(style_token,df[\"pp_style\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(iid_token,df[\"iid\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(iid_token,df[\"p_iid\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(iid_token,df[\"pp_iid\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(alt_token,df[\"alt\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(src_token,df[\"src\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(href_token,df[\"href\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(text_token,df[\"text\"])], axis=1)\n",
        "  X = pd.concat([X, to_df(text_token,df[\"p_text\"])], axis=1)\n",
        "  X = X.fillna(0)\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X4d6XPR75_bT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## tokenの作成"
      ]
    },
    {
      "metadata": {
        "id": "erxChBQj302A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "temp_df = df[\"tag\"].append(df[\"p_tag\"]).append(df[\"pp_tag\"])\n",
        "tag_token = Tokenizer(lower=True)\n",
        "tag_token.fit_on_texts(temp_df)\n",
        "\n",
        "temp_df = df[\"iid\"].append(df[\"p_iid\"]).append(df[\"pp_iid\"])\n",
        "iid_token = Tokenizer(100,lower=True)\n",
        "iid_token.fit_on_texts(temp_df)\n",
        "\n",
        "temp_df = df[\"style\"].append(df[\"p_style\"]).append(df[\"pp_style\"])\n",
        "style_token = Tokenizer(100,lower=True)\n",
        "style_token.fit_on_texts(temp_df)\n",
        "\n",
        "alt_token = Tokenizer(100,lower=True)\n",
        "alt_token.fit_on_texts(df[\"alt\"])\n",
        "src_token = Tokenizer(100,lower=True)\n",
        "src_token.fit_on_texts(df[\"src\"])\n",
        "href_token = Tokenizer(100,lower=True)\n",
        "href_token.fit_on_texts(df[\"href\"])\n",
        "\n",
        "temp_df = df[\"text\"].append(df[\"p_text\"])\n",
        "text_token = Tokenizer(500,lower=True)\n",
        "text_token.fit_on_texts(df[\"text\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKmpnu8L8Q32",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##学習データを作成"
      ]
    },
    {
      "metadata": {
        "id": "ugzkP54t8lr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "e0923cf7-5ed4-4d69-de55-1609fbcdc591"
      },
      "cell_type": "code",
      "source": [
        "X = prepare_data(df)\n",
        "print(X.shape)\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4489, 2050)\n",
            "height       int64\n",
            "width        int64\n",
            "x            int64\n",
            "y            int64\n",
            "p_x          int64\n",
            "p_y          int64\n",
            "p_width      int64\n",
            "p_height     int64\n",
            "pp_height    int64\n",
            "pp_width     int64\n",
            "pp_x         int64\n",
            "pp_y         int64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G7TNtGUqsJy6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ラベルを作成"
      ]
    },
    {
      "metadata": {
        "id": "jpH6sOY-zc7t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b21fc4b8-8458-49d6-a3da-9263997b61d2"
      },
      "cell_type": "code",
      "source": [
        "label_token = Tokenizer()\n",
        "label_token.fit_on_texts(df[\"label\"])\n",
        "save_text_tokenizer(label_token,base_dir+\"label_token\")\n",
        "y = label_token.texts_to_matrix(df[\"label\"])\n",
        "print(y.shape)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4489, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5iJ7es73GIA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## データ(型)を確認"
      ]
    },
    {
      "metadata": {
        "id": "03hPJrJHwIRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4d9c2b40-7648-4186-ea6d-473b4ece24db"
      },
      "cell_type": "code",
      "source": [
        "print(X[[\"height\",\"width\",\"x\",\"y\",\"p_x\",\"p_y\",\"p_width\",\"p_height\",\"pp_height\",\"pp_width\",\"pp_x\",\"pp_y\"]].dtypes)\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "height       int64\n",
            "width        int64\n",
            "x            int64\n",
            "y            int64\n",
            "p_x          int64\n",
            "p_y          int64\n",
            "p_width      int64\n",
            "p_height     int64\n",
            "pp_height    int64\n",
            "pp_width     int64\n",
            "pp_x         int64\n",
            "pp_y         int64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "93-RXzxesw40",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ラベルの変換用ディクショナリの作成"
      ]
    },
    {
      "metadata": {
        "id": "NBK6zPdziu6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b318f150-f266-445a-d8cf-315ed1605d89"
      },
      "cell_type": "code",
      "source": [
        "label_dic= {v: k for k, v in label_token.word_index.items()}\n",
        "print(label_dic)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'skip', 2: 'vari'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kabh_T6dtFV8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 学習データをシャッフル"
      ]
    },
    {
      "metadata": {
        "id": "9CzGifwVqJmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.33, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYBAjvDN8nRa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## コールバックの設定"
      ]
    },
    {
      "metadata": {
        "id": "qfcultG18oTA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# stop when score is not improve\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# save best score\n",
        "filepath = base_dir+\"weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EGLTH5gYtRes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 学習開始"
      ]
    },
    {
      "metadata": {
        "id": "s-_DaxTjGVmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2672
        },
        "outputId": "9b12cf8b-ae29-41c5-bf7e-650fb8e468c6"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 275\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1024, activation='relu',\n",
        "                       input_shape=(X_train.shape[1],)))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(160, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['categorical_accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train,verbose=1, validation_data=(X_test,y_test), epochs=EPOCHS, batch_size=BATCH_SIZE,callbacks=callbacks_list)\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history.keys())\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['val_loss', 'loss'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['val_acc', 'acc'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3007 samples, validate on 1482 samples\n",
            "Epoch 1/100\n",
            "3007/3007 [==============================] - 2s 792us/step - loss: 0.9204 - categorical_accuracy: 0.7083 - val_loss: 0.2821 - val_categorical_accuracy: 0.8927\n",
            "\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.89271, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 2/100\n",
            "3007/3007 [==============================] - 2s 582us/step - loss: 0.3320 - categorical_accuracy: 0.8766 - val_loss: 0.2097 - val_categorical_accuracy: 0.8974\n",
            "\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.89271 to 0.89744, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 3/100\n",
            "3007/3007 [==============================] - 2s 582us/step - loss: 0.2531 - categorical_accuracy: 0.9162 - val_loss: 0.1699 - val_categorical_accuracy: 0.9480\n",
            "\n",
            "Epoch 00003: val_categorical_accuracy improved from 0.89744 to 0.94804, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 4/100\n",
            "3007/3007 [==============================] - 2s 593us/step - loss: 0.1751 - categorical_accuracy: 0.9182 - val_loss: 0.1530 - val_categorical_accuracy: 0.9386\n",
            "\n",
            "Epoch 00004: val_categorical_accuracy did not improve from 0.94804\n",
            "Epoch 5/100\n",
            "3007/3007 [==============================] - 2s 574us/step - loss: 0.1499 - categorical_accuracy: 0.9421 - val_loss: 0.1059 - val_categorical_accuracy: 0.9190\n",
            "\n",
            "Epoch 00005: val_categorical_accuracy did not improve from 0.94804\n",
            "Epoch 6/100\n",
            "3007/3007 [==============================] - 2s 575us/step - loss: 0.1560 - categorical_accuracy: 0.9478 - val_loss: 0.1125 - val_categorical_accuracy: 0.9737\n",
            "\n",
            "Epoch 00006: val_categorical_accuracy improved from 0.94804 to 0.97368, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 7/100\n",
            "3007/3007 [==============================] - 2s 591us/step - loss: 0.1409 - categorical_accuracy: 0.9465 - val_loss: 0.1084 - val_categorical_accuracy: 0.9676\n",
            "\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.97368\n",
            "Epoch 8/100\n",
            "3007/3007 [==============================] - 2s 592us/step - loss: 0.1350 - categorical_accuracy: 0.9495 - val_loss: 0.0873 - val_categorical_accuracy: 0.9642\n",
            "\n",
            "Epoch 00008: val_categorical_accuracy did not improve from 0.97368\n",
            "Epoch 9/100\n",
            "3007/3007 [==============================] - 2s 579us/step - loss: 0.1246 - categorical_accuracy: 0.9514 - val_loss: 0.1026 - val_categorical_accuracy: 0.9642\n",
            "\n",
            "Epoch 00009: val_categorical_accuracy did not improve from 0.97368\n",
            "Epoch 10/100\n",
            "3007/3007 [==============================] - 2s 584us/step - loss: 0.1139 - categorical_accuracy: 0.9578 - val_loss: 0.0847 - val_categorical_accuracy: 0.9663\n",
            "\n",
            "Epoch 00010: val_categorical_accuracy did not improve from 0.97368\n",
            "Epoch 11/100\n",
            "3007/3007 [==============================] - 2s 574us/step - loss: 0.0941 - categorical_accuracy: 0.9634 - val_loss: 0.0633 - val_categorical_accuracy: 0.9757\n",
            "\n",
            "Epoch 00011: val_categorical_accuracy improved from 0.97368 to 0.97571, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 12/100\n",
            "3007/3007 [==============================] - 2s 592us/step - loss: 0.1006 - categorical_accuracy: 0.9628 - val_loss: 0.0749 - val_categorical_accuracy: 0.9744\n",
            "\n",
            "Epoch 00012: val_categorical_accuracy did not improve from 0.97571\n",
            "Epoch 13/100\n",
            "3007/3007 [==============================] - 2s 576us/step - loss: 0.0934 - categorical_accuracy: 0.9651 - val_loss: 0.0474 - val_categorical_accuracy: 0.9912\n",
            "\n",
            "Epoch 00013: val_categorical_accuracy improved from 0.97571 to 0.99123, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 14/100\n",
            "3007/3007 [==============================] - 2s 580us/step - loss: 0.0734 - categorical_accuracy: 0.9684 - val_loss: 0.0455 - val_categorical_accuracy: 0.9798\n",
            "\n",
            "Epoch 00014: val_categorical_accuracy did not improve from 0.99123\n",
            "Epoch 15/100\n",
            "3007/3007 [==============================] - 2s 570us/step - loss: 0.0944 - categorical_accuracy: 0.9628 - val_loss: 0.0545 - val_categorical_accuracy: 0.9696\n",
            "\n",
            "Epoch 00015: val_categorical_accuracy did not improve from 0.99123\n",
            "Epoch 16/100\n",
            "3007/3007 [==============================] - 2s 570us/step - loss: 0.0668 - categorical_accuracy: 0.9747 - val_loss: 0.0532 - val_categorical_accuracy: 0.9811\n",
            "\n",
            "Epoch 00016: val_categorical_accuracy did not improve from 0.99123\n",
            "Epoch 17/100\n",
            "3007/3007 [==============================] - 2s 566us/step - loss: 0.0645 - categorical_accuracy: 0.9751 - val_loss: 0.1054 - val_categorical_accuracy: 0.9582\n",
            "\n",
            "Epoch 00017: val_categorical_accuracy did not improve from 0.99123\n",
            "Epoch 18/100\n",
            "3007/3007 [==============================] - 2s 593us/step - loss: 0.0741 - categorical_accuracy: 0.9711 - val_loss: 0.0336 - val_categorical_accuracy: 0.9879\n",
            "\n",
            "Epoch 00018: val_categorical_accuracy did not improve from 0.99123\n",
            "Epoch 19/100\n",
            "3007/3007 [==============================] - 2s 570us/step - loss: 0.0592 - categorical_accuracy: 0.9810 - val_loss: 0.0301 - val_categorical_accuracy: 0.9892\n",
            "\n",
            "Epoch 00019: val_categorical_accuracy did not improve from 0.99123\n",
            "Epoch 20/100\n",
            "3007/3007 [==============================] - 2s 559us/step - loss: 0.0469 - categorical_accuracy: 0.9771 - val_loss: 0.0320 - val_categorical_accuracy: 0.9933\n",
            "\n",
            "Epoch 00020: val_categorical_accuracy improved from 0.99123 to 0.99325, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 21/100\n",
            "3007/3007 [==============================] - 2s 596us/step - loss: 0.0492 - categorical_accuracy: 0.9804 - val_loss: 0.0282 - val_categorical_accuracy: 0.9906\n",
            "\n",
            "Epoch 00021: val_categorical_accuracy did not improve from 0.99325\n",
            "Epoch 22/100\n",
            "3007/3007 [==============================] - 2s 603us/step - loss: 0.0497 - categorical_accuracy: 0.9787 - val_loss: 0.0316 - val_categorical_accuracy: 0.9879\n",
            "\n",
            "Epoch 00022: val_categorical_accuracy did not improve from 0.99325\n",
            "Epoch 23/100\n",
            "3007/3007 [==============================] - 2s 602us/step - loss: 0.0373 - categorical_accuracy: 0.9837 - val_loss: 0.0243 - val_categorical_accuracy: 0.9980\n",
            "\n",
            "Epoch 00023: val_categorical_accuracy improved from 0.99325 to 0.99798, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 24/100\n",
            "3007/3007 [==============================] - 2s 598us/step - loss: 0.0373 - categorical_accuracy: 0.9847 - val_loss: 0.0169 - val_categorical_accuracy: 0.9933\n",
            "\n",
            "Epoch 00024: val_categorical_accuracy did not improve from 0.99798\n",
            "Epoch 25/100\n",
            "3007/3007 [==============================] - 2s 605us/step - loss: 0.0298 - categorical_accuracy: 0.9850 - val_loss: 0.0175 - val_categorical_accuracy: 0.9946\n",
            "\n",
            "Epoch 00025: val_categorical_accuracy did not improve from 0.99798\n",
            "Epoch 26/100\n",
            "3007/3007 [==============================] - 2s 592us/step - loss: 0.0266 - categorical_accuracy: 0.9864 - val_loss: 0.0146 - val_categorical_accuracy: 0.9966\n",
            "\n",
            "Epoch 00026: val_categorical_accuracy did not improve from 0.99798\n",
            "Epoch 27/100\n",
            "3007/3007 [==============================] - 2s 599us/step - loss: 0.0316 - categorical_accuracy: 0.9840 - val_loss: 0.0148 - val_categorical_accuracy: 0.9973\n",
            "\n",
            "Epoch 00027: val_categorical_accuracy did not improve from 0.99798\n",
            "Epoch 28/100\n",
            "3007/3007 [==============================] - 2s 599us/step - loss: 0.0234 - categorical_accuracy: 0.9884 - val_loss: 0.0158 - val_categorical_accuracy: 0.9960\n",
            "\n",
            "Epoch 00028: val_categorical_accuracy did not improve from 0.99798\n",
            "Epoch 29/100\n",
            "3007/3007 [==============================] - 2s 628us/step - loss: 0.0213 - categorical_accuracy: 0.9900 - val_loss: 0.0162 - val_categorical_accuracy: 0.9953\n",
            "\n",
            "Epoch 00029: val_categorical_accuracy did not improve from 0.99798\n",
            "Epoch 30/100\n",
            "3007/3007 [==============================] - 2s 567us/step - loss: 0.0156 - categorical_accuracy: 0.9930 - val_loss: 0.0149 - val_categorical_accuracy: 0.9987\n",
            "\n",
            "Epoch 00030: val_categorical_accuracy improved from 0.99798 to 0.99865, saving model to /content/gdrive/My Drive/data/stock_filter/018/weights.best.hdf5\n",
            "Epoch 31/100\n",
            "3007/3007 [==============================] - 2s 571us/step - loss: 0.0228 - categorical_accuracy: 0.9884 - val_loss: 0.0123 - val_categorical_accuracy: 0.9987\n",
            "\n",
            "Epoch 00031: val_categorical_accuracy did not improve from 0.99865\n",
            "Epoch 32/100\n",
            "3007/3007 [==============================] - 2s 560us/step - loss: 0.0163 - categorical_accuracy: 0.9924 - val_loss: 0.0125 - val_categorical_accuracy: 0.9980\n",
            "\n",
            "Epoch 00032: val_categorical_accuracy did not improve from 0.99865\n",
            "Epoch 33/100\n",
            "3007/3007 [==============================] - 2s 567us/step - loss: 0.0178 - categorical_accuracy: 0.9937 - val_loss: 0.0127 - val_categorical_accuracy: 0.9966\n",
            "\n",
            "Epoch 00033: val_categorical_accuracy did not improve from 0.99865\n",
            "Epoch 34/100\n",
            "3007/3007 [==============================] - 2s 563us/step - loss: 0.0176 - categorical_accuracy: 0.9894 - val_loss: 0.0145 - val_categorical_accuracy: 0.9960\n",
            "\n",
            "Epoch 00034: val_categorical_accuracy did not improve from 0.99865\n",
            "Epoch 35/100\n",
            "3007/3007 [==============================] - 2s 561us/step - loss: 0.0142 - categorical_accuracy: 0.9947 - val_loss: 0.0104 - val_categorical_accuracy: 0.9980\n",
            "\n",
            "Epoch 00035: val_categorical_accuracy did not improve from 0.99865\n",
            "Epoch 36/100\n",
            "3007/3007 [==============================] - 2s 571us/step - loss: 0.0097 - categorical_accuracy: 0.9957 - val_loss: 0.0127 - val_categorical_accuracy: 0.9973\n",
            "\n",
            "Epoch 00036: val_categorical_accuracy did not improve from 0.99865\n",
            "Epoch 37/100\n",
            "3007/3007 [==============================] - 2s 581us/step - loss: 0.0155 - categorical_accuracy: 0.9933 - val_loss: 0.0111 - val_categorical_accuracy: 0.9980\n",
            "\n",
            "Epoch 00037: val_categorical_accuracy did not improve from 0.99865\n",
            "Epoch 38/100\n",
            "3007/3007 [==============================] - 2s 565us/step - loss: 0.0105 - categorical_accuracy: 0.9953 - val_loss: 0.0103 - val_categorical_accuracy: 0.9980\n",
            "\n",
            "Epoch 00038: val_categorical_accuracy did not improve from 0.99865\n",
            "Epoch 39/100\n",
            " 550/3007 [====>.........................] - ETA: 1s - loss: 0.0072 - categorical_accuracy: 0.9964"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KqcbsqV4zbP3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1024, activation='relu',\n",
        "                       input_shape=(X_train.shape[1],)))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(160, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1024, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(y_train.shape[1],activation='softmax'))```\n",
        "\n",
        "\n",
        "\n",
        "Test accuracy: 0.9949977262391997\n"
      ]
    },
    {
      "metadata": {
        "id": "wo_ZPvqiraTF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## tokenの保存"
      ]
    },
    {
      "metadata": {
        "id": "f3m3ACO7quTX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_text_tokenizer(tag_token,base_dir+\"tag_token\")\n",
        "save_text_tokenizer(iid_token,base_dir+\"iid_token\")\n",
        "save_text_tokenizer(style_token,base_dir+\"style_token\")\n",
        "save_text_tokenizer(href_token,base_dir+\"href_token\")\n",
        "save_text_tokenizer(src_token,base_dir+\"src_token\")\n",
        "save_text_tokenizer(alt_token,base_dir+\"alt_token\")\n",
        "save_text_tokenizer(text_token,base_dir+\"text_token\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7jZmfCpOp55w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenisor を読み込む"
      ]
    },
    {
      "metadata": {
        "id": "wvDYztzIQB4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tag_token = load_text_tokenizer(base_dir+\"tag_token\")\n",
        "iid_token = load_text_tokenizer(base_dir+\"iid_token\")\n",
        "style_token = load_text_tokenizer(base_dir+\"style_token\")\n",
        "href_token = load_text_tokenizer(base_dir+\"href_token\")\n",
        "src_token = load_text_tokenizer(base_dir+\"src_token\")\n",
        "alt_token = load_text_tokenizer(base_dir+\"alt_token\")\n",
        "text_token = load_text_tokenizer(base_dir+\"text_token\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c54ev9lpv7d_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## モデルと重みを保存する"
      ]
    },
    {
      "metadata": {
        "id": "eAWThi4VQmag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###モデルを保存\n"
      ]
    },
    {
      "metadata": {
        "id": "0LMxMza9xnwS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open(base_dir + \"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "    \n",
        "with open(base_dir + \"model.h5\",'w') as model_file: \n",
        "  model.save_weights(base_dir+\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RFIR9rNMqH1d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###モデルを読み込み"
      ]
    },
    {
      "metadata": {
        "id": "KbaD369ozNA-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load\n",
        "with open(base_dir+'model.json', 'r') as json_file:\n",
        "  loaded_model_json = json_file.read()\n",
        "  loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "with open(base_dir+\"model.h5\",'r') as model_file: \n",
        "  loaded_model.load_weights(base_dir+\"model.h5\")\n",
        "  print(\"Loaded model from disk\")\n",
        "\n",
        "# compile\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['categorical_accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWofndFMKU7B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###モデルの復元が正確かを確認"
      ]
    },
    {
      "metadata": {
        "id": "BVOlQsvhCa4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vI-u1U_0r_w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 実際のデータでテスト"
      ]
    },
    {
      "metadata": {
        "id": "kRayunPj0yAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}